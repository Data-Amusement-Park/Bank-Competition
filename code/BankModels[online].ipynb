{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prepare"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install algo-timer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import lib\nimport pandas as pd\nimport numpy as np\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import roc_auc_score, confusion_matrix, roc_curve, auc\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import balanced_accuracy_score\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn import svm\nfrom imblearn.ensemble import EasyEnsembleClassifier, BalancedBaggingClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib as mpl\nmpl.style.use('fivethirtyeight')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nfrom xgboost import plot_importance\nfrom sklearn.metrics import make_scorer\nimport time\nfrom hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK, STATUS_RUNNING","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport gc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from algotimer import Timer, TimerPloter","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"import pandas as pd\ntestX = pd.read_csv(\"../input/testX.csv\")\nX = pd.read_csv(\"../input/trainX.csv\")\ny = pd.read_csv(\"../input/trainY.csv\", header=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ids = testX.id\ntestX = testX.drop(['id'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ROC\ndef plotROC(y_test, y_score, pltName):\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n    fpr[2], tpr[2], _ = roc_curve(y_test, y_score)\n    roc_auc[2] = auc(fpr[2], tpr[2])\n\n    # Compute micro-average ROC curve and ROC area\n    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test, y_score)\n    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n    # plot it\n    plt.figure()\n    lw = 2\n    plt.plot(fpr[2], tpr[2], color='darkorange',\n             lw=lw, label='AUC=%0.2f' % roc_auc[2])\n    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('FPR')\n    plt.ylabel('TPR')\n    plt.title('ROC of ' + pltName)\n    plt.legend(loc=\"best\")\n    plt.savefig(f'{pltName}.png', bbox_inches='tight', dpi=300)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, stratify=y, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"def objective(params):\n    time1 = time.time()\n    params = {\n        'max_depth': int(params['max_depth']),\n        'gamma': \"{:.3f}\".format(params['gamma']),\n        'subsample': \"{:.2f}\".format(params['subsample']),\n        'reg_alpha': \"{:.3f}\".format(params['reg_alpha']),\n        'reg_lambda': \"{:.3f}\".format(params['reg_lambda']),\n        'learning_rate': \"{:.3f}\".format(params['learning_rate']),\n        'num_leaves': '{:.3f}'.format(params['num_leaves']),\n        'colsample_bytree': '{:.3f}'.format(params['colsample_bytree']),\n        'min_child_samples': '{:.3f}'.format(params['min_child_samples']),\n        'feature_fraction': '{:.3f}'.format(params['feature_fraction']),\n        'bagging_fraction': '{:.3f}'.format(params['bagging_fraction'])\n    }\n\n    print(\"\\n############## New Run ################\")\n    print(f\"params = {params}\")\n    FOLDS = 5\n    count = 1\n    skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42)\n    score_mean = 0\n    for tr_idx, val_idx in skf.split(X_train, y_train.values.ravel()):\n        clf = xgb.XGBClassifier(\n            objective=\"binary:logistic\",\n            n_estimators=300, random_state=4, verbose=True, \n            tree_method='hist', \n            scale_pos_weight=136,\n            n_jobs=-1,\n            **params\n        )\n\n        X_tr, X_vl = X_train.iloc[tr_idx, :], X_train.iloc[val_idx, :]\n        y_tr, y_vl = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n        \n        clf.fit(X_tr, y_tr.values.ravel())\n        score = make_scorer(roc_auc_score, needs_proba=True)(clf, X_vl, y_vl)\n        score_mean += score\n        print(f'{count} CV - score: {round(score, 4)}')\n        count += 1\n    time2 = time.time() - time1\n    print(f\"Total Time Run: {round(time2 / 60,2)}\")\n    gc.collect()\n    print(f'Mean ROC_AUC: {score_mean / FOLDS}')\n    del X_tr, X_vl, y_tr, y_vl, clf, score\n    return -(score_mean / FOLDS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"space = {\n    # The maximum depth of a tree, same as GBM.\n    # Used to control over-fitting as higher depth will allow model \n    # to learn relations very specific to a particular sample.\n    # Should be tuned using CV.\n    # Typical values: 3-10\n    'max_depth': hp.quniform('max_depth', 2, 6, 1),\n    \n    # reg_alpha: L1 regularization term. L1 regularization encourages sparsity \n    # (meaning pulling weights to 0). It can be more useful when the objective\n    # is logistic regression since you might need help with feature selection.\n    'reg_alpha':  hp.uniform('reg_alpha', 0.01, 0.4),\n    \n    # reg_lambda: L2 regularization term. L2 encourages smaller weights, this\n    # approach can be more useful in tree-models where zeroing \n    # features might not make much sense.\n    'reg_lambda': hp.uniform('reg_lambda', 0.01, 0.4),\n    \n    # eta: Analogous to learning rate in GBM\n    # Makes the model more robust by shrinking the weights on each step\n    # Typical final values to be used: 0.01-0.2\n    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n    \n    # colsample_bytree: Similar to max_features in GBM. Denotes the \n    # fraction of columns to be randomly samples for each tree.\n    # Typical values: 0.5-1\n    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1),\n    \n    # A node is split only when the resulting split gives a positive\n    # reduction in the loss function. Gamma specifies the \n    # minimum loss reduction required to make a split.\n    # Makes the algorithm conservative. The values can vary depending on the loss function and should be tuned.\n    'gamma': hp.uniform('gamma', 0.01, .7),\n    \n    # more increases accuracy, but may lead to overfitting.\n    # num_leaves: the number of leaf nodes to use. Having a large number \n    # of leaves will improve accuracy, but will also lead to overfitting.\n    'num_leaves': hp.choice('num_leaves', list(range(10, 100, 10))),\n    \n    # specifies the minimum samples per leaf node.\n    # the minimum number of samples (data) to group into a leaf. \n    # The parameter can greatly assist with overfitting: larger sample\n    # sizes per leaf will reduce overfitting (but may lead to under-fitting).\n    'min_child_samples': hp.choice('min_child_samples', list(range(10, 200, 20))),\n    \n    # subsample: represents a fraction of the rows (observations) to be \n    # considered when building each subtree. Tianqi Chen and Carlos Guestrin\n    # in their paper A Scalable Tree Boosting System recommend \n    'subsample': hp.choice('subsample', [0.7, 0.8, 0.9, 1]),\n    \n    # randomly select a fraction of the features.\n    # feature_fraction: controls the subsampling of features used\n    # for training (as opposed to subsampling the actual training data in \n    # the case of bagging). Smaller fractions reduce overfitting.\n    'feature_fraction': hp.uniform('feature_fraction', 0.7, 1),\n    \n    # randomly bag or subsample training data.\n    'bagging_fraction': hp.uniform('bagging_fraction', 0.7, 1)\n    \n    # bagging_fraction and bagging_freq: enables bagging (subsampling) \n    # of the training data. Both values need to be set for bagging to be used.\n    # The frequency controls how often (iteration) bagging is used. Smaller\n    # fractions and frequencies reduce overfitting.\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set algoritm parameters\nwith Timer('XGBoost, Search') as t:\n    best = fmin(fn=objective,\n                space=space,\n                algo=tpe.suggest,\n                max_evals=20)\n    best_params = space_eval(space, best)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Print best parameters\nbest_params['max_depth'] = int(best_params['max_depth'])\nprint(\"BEST PARAMS: \", best_params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = xgb.XGBClassifier(\n    n_estimators=300,\n    **best_params,\n    tree_method='hist',\n    eval_metric=\"auc\",\n    n_jobs=-1,\n    scale_pos_weight=136\n)\n\nclf.fit(X_train, y_train.values.ravel())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_y_train_pred = clf.predict_proba(X_train)[:,1]\nplotROC(y_train, xgb_y_train_pred, 'XGBoost-Train')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_y_test_pred = clf.predict_proba(X_test)[:,1]\nplotROC(y_test, xgb_y_test_pred, 'XGBoost-Test')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train with all data\nwith Timer('XGBoost, Train') as t:\n    clf.fit(X, y.values.ravel(), verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train auc\nxgb_y_all_pred = clf.predict_proba(X)[:,1]\nroc_auc_score(y, xgb_y_all_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotROC(y, xgb_y_all_pred, 'XGBoost-Train-AllData')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = pd.DataFrame()\nresult['id'] = ids\nwith Timer('XGBoost, Prediction') as t:\n    result['target'] = clf.predict_proba(testX)[:,1]\nresult.to_csv('xgb.csv', index=None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Easy Ensemble Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"def objectiveEasy(params):\n    time1 = time.time()\n    params = {\n        'sampling_strategy': params['sampling_strategy'],\n    }\n\n    print(\"\\n############## New Run ################\")\n    print(f\"params = {params}\")\n    FOLDS = 5\n    count = 1\n    skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42)\n    score_mean = 0\n    for tr_idx, val_idx in skf.split(X_train, y_train.values.ravel()):\n        clf = EasyEnsembleClassifier(**params,\n                                    random_state=0,\n                                    n_estimators=300,\n                                    n_jobs=-1,\n                                    verbose=0)\n\n        X_tr, X_vl = X_train.iloc[tr_idx, :], X_train.iloc[val_idx, :]\n        y_tr, y_vl = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n        \n        clf.fit(X_tr, y_tr.values.ravel())\n        score = make_scorer(roc_auc_score, needs_proba=True)(clf, X_vl, y_vl)\n        score_mean += score\n        print(f'{count} CV - score: {round(score, 4)}')\n        count += 1\n    time2 = time.time() - time1\n    print(f\"Total Time Run: {round(time2 / 60,2)}\")\n    gc.collect()\n    print(f'Mean ROC_AUC: {score_mean / FOLDS}')\n    del X_tr, X_vl, y_tr, y_vl, clf, score\n    return -(score_mean / FOLDS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spaceEasy = {\n    'sampling_strategy': hp.choice('sampling_strategy', [0.7, 0.8, 0.9, 1.0, 'auto'])\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set algoritm parameters\nwith Timer('EasyEnsamble, Search') as t:\n    bestEasy = fmin(fn=objectiveEasy,\n                space=spaceEasy,\n                algo=tpe.suggest,\n                max_evals=5)\n\n    # Print best parameters\n    bestEasy_params = space_eval(spaceEasy, bestEasy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bestEasy_params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = EasyEnsembleClassifier(**bestEasy_params,\n                            random_state=0,\n                            n_estimators=300,\n                            n_jobs=-1,\n                            verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# training roc\neasy_y_train_pred = clf.predict_proba(X_train)[:,1]\nplotROC(y_train, easy_y_train_pred, 'EasyEnsamble-Train')\n# test roc\neasy_y_test_pred = clf.predict_proba(X_test)[:,1]\nplotROC(y_test, easy_y_test_pred, 'EasyEnsamble-Test')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit all data\nwith Timer('EasyEnsamble, Train') as t:\n    clf.fit(X, y.values.ravel())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"easy_y_all_pred = clf.predict_proba(X)[:, 1]\nplotROC(y, easy_y_all_pred, 'EasyEnsamble-Train-AllData')\nroc_auc_score(y, easy_y_all_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pridict\nresult = pd.DataFrame()\nresult['id'] = ids\nwith Timer('EasyEnsamble, Prediction') as t:\n    result['target'] = clf.predict_proba(testX)[:, 1]\nresult.to_csv('EasyEnsemble.csv', index=None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Balanced Bagging Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"def objectiveBalance(params):\n    time1 = time.time()\n    params = {\n        'sampling_strategy': params['sampling_strategy'],\n    }\n\n    print(\"\\n############## New Run ################\")\n    print(f\"params = {params}\")\n    FOLDS = 5\n    count = 1\n    skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42)\n    score_mean = 0\n    for tr_idx, val_idx in skf.split(X_train, y_train.values.ravel()):\n        clf = BalancedBaggingClassifier(**params,\n                                    random_state=0,\n                                    n_estimators=300,\n                                    n_jobs=-1,\n                                    verbose=0)\n\n        X_tr, X_vl = X_train.iloc[tr_idx, :], X_train.iloc[val_idx, :]\n        y_tr, y_vl = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n        \n        clf.fit(X_tr, y_tr.values.ravel())\n        score = make_scorer(roc_auc_score, needs_proba=True)(clf, X_vl, y_vl)\n        score_mean += score\n        print(f'{count} CV - score: {round(score, 4)}')\n        count += 1\n    time2 = time.time() - time1\n    print(f\"Total Time Run: {round(time2 / 60,2)}\")\n    gc.collect()\n    print(f'Mean ROC_AUC: {score_mean / FOLDS}')\n    del X_tr, X_vl, y_tr, y_vl, clf, score\n    return -(score_mean / FOLDS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spaceBalance = spaceEasy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with Timer('BalancedEnsamble, Search') as t:\n    # Set algoritm parameters\n    bestBalance = fmin(fn=objectiveBalance,\n                space=spaceBalance,\n                algo=tpe.suggest,\n                max_evals=5)\n  \n    # Print best parameters\n    bestBalance_params = space_eval(spaceBalance, bestBalance)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = BalancedBaggingClassifier(**bestBalance_params,\n                                random_state=0,\n                                n_estimators=300,\n                                n_jobs=-1,\n                                verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# training roc\nbalance_y_train_pred = clf.predict_proba(X_train)[:,1]\nplotROC(y_train, balance_y_train_pred, 'BalancedEnsamble-Train')\n# test roc\nbalance_y_test_pred = clf.predict_proba(X_test)[:,1]\nplotROC(y_test, balance_y_test_pred, 'BalancedEnsamble-Test')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with Timer('BalancedEnsamble, Train') as t:\n    # train with all data\n    clf.fit(X, y.values.ravel())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"balance_y_all_pred = clf.predict_proba(X)[:, 1]\nplotROC(y, balance_y_all_pred, 'BalancedEnsamble-Train-AllData')\nroc_auc_score(y, balance_y_all_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = pd.DataFrame()\nresult['id'] = ids\nwith Timer('BalancedEnsamble, Prediction') as t:\n    result['target'] = clf.predict_proba(testX)[:, 1]\nresult.to_csv('BalancedBaggingClassifier.csv', index=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def subplotRoc(y_test, y_score, pltName):\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n    fpr[2], tpr[2], _ = roc_curve(y_test, y_score)\n    roc_auc[2] = auc(fpr[2], tpr[2])\n\n    # Compute micro-average ROC curve and ROC area\n    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test, y_score)\n    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n    # plot it\n    lw = 2\n    plt.plot(fpr[2], tpr[2], color='darkorange',\n             lw=lw, label='AUC=%0.2f' % roc_auc[2])\n    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('FPR')\n    plt.ylabel('TPR')\n    plt.title('ROC of ' + pltName)\n    plt.legend(loc=\"best\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 3 x 3 ROC plot\nfig = plt.subplots(3, 3, figsize=(16, 14))\nplt.subplots_adjust(hspace=0.6)\nplt.suptitle('ROC of All')\nplt.subplot(331)\nsubplotRoc(y_train, xgb_y_train_pred, 'XGBoost-Train')\nplt.subplot(332)\nsubplotRoc(y_test, xgb_y_test_pred, 'XGBoost-Test')\nplt.subplot(333)\nsubplotRoc(y, xgb_y_all_pred, 'XGBoost-Train-All-Data')\nplt.subplot(334)\nsubplotRoc(y_train, easy_y_train_pred, 'EasyEnsamble-Train')\nplt.subplot(335)\nsubplotRoc(y_test, easy_y_test_pred, 'EasyEnsamble-Test')\nplt.subplot(336)\nsubplotRoc(y, easy_y_all_pred, 'EasyEnsamble-Train-AllData')\nplt.subplot(337)\nsubplotRoc(y_train, balance_y_train_pred, 'BalancedEnsamble-Train')\nplt.subplot(338)\nsubplotRoc(y_test, balance_y_test_pred, 'BalancedEnsamble-Test')\nplt.subplot(339)\nsubplotRoc(y, balance_y_all_pred, 'BalancedEnsamble-Train-AllData')\n\nplt.savefig('ROC.png', bbox_inches='tight', dpi=300)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}